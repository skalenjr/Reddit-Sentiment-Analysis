{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sentiment Analysis on Reddit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subreddit to perform sentiment analysis on:\n",
    "subreddit = 'pcgaming'\n",
    "# Name of file that contains the topics to search the subreddit for.\n",
    "topic_file = 'topic_list.csv'\n",
    "# Number of posts to search for topics in.\n",
    "num_posts = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from difflib import SequenceMatcher\n",
    "import statistics\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import bot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_fuzzy(post_text, topic_df):\n",
    "    topic = ''\n",
    "    for t in topic_df['Topic']:\n",
    "        t = str(t)\n",
    "        t_split = t.split()\n",
    "        topic_len = len(t_split)\n",
    "        post_list = post_text.split()\n",
    "        ngram = nltk.ngrams(post_list, topic_len)\n",
    "        for grams in ngram:\n",
    "            r_list = [0] * len(grams)\n",
    "            for i, word in enumerate(t_split):\n",
    "                r_list[i] = fuzz.ratio(word, grams[i])\n",
    "            if statistics.mean(r_list) >= 90:\n",
    "                return ' '.join(t_split)\n",
    "    return topic\n",
    "\n",
    "def get_topic_first(post_text, topic_df):\n",
    "    topic = ''\n",
    "    for t in topic_df['Topic']:\n",
    "        t = str(t)\n",
    "        t_split = t.split()\n",
    "        topic_len = len(t_split)\n",
    "        post_list = post_text.split()\n",
    "        ngram = nltk.ngrams(post_list, topic_len)\n",
    "        for grams in ngram:\n",
    "            print(grams)\n",
    "            exit()\n",
    "    return topic\n",
    "\n",
    "def get_topic_exact(post_text, topic_df):\n",
    "    topic = ''\n",
    "    for t in topic_df['Topic']:\n",
    "        t = str(t)\n",
    "        t_split = t.split()\n",
    "        topic_len = len(t_split)\n",
    "        post_list = post_text.split()\n",
    "        ngram = nltk.ngrams(post_list, topic_len)\n",
    "        for grams in ngram:\n",
    "            if list(grams) == t_split:\n",
    "                return ' '.join(t_split)\n",
    "    return topic\n",
    "\n",
    "def get_url(postID):\n",
    "    return 'reddit.com/r/' + subreddit + '/comments/' + postID\n",
    "\n",
    "reddit = praw.Reddit(client_id=bot1.app_id, client_secret=bot1.app_secret, user_agent=bot1.app_ua)\n",
    "sub = reddit.subreddit(subreddit)\n",
    "topic_df = pd.read_csv(topic_file)\n",
    "posts = pd.DataFrame(columns=['PostID', 'Title', 'Topic'])\n",
    "\n",
    "for post in sub.hot(limit=num_posts):\n",
    "    url = get_url(post.id)\n",
    "    title = post.title\n",
    "    #topic = get_topic_fuzzy(title, topic_df)\n",
    "    #topic = get_topic_first(title, topic_df)\n",
    "    topic = get_topic_exact(title, topic_df)\n",
    "    if topic != '':\n",
    "        posts.loc[len(posts.index)] = [post.id, title, topic]\n",
    "\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(ID):\n",
    "    pass\n",
    "    post = reddit.submission(id=ID)\n",
    "    all_comments = []\n",
    "    post.comments.replace_more(limit=None)\n",
    "    for comments in post.comments.list():\n",
    "        all_comments.append(comments.body)\n",
    "        #all_comments.append(nltk.tokenize.sent_tokenize(comments.body))\n",
    "    return pd.DataFrame(all_comments, columns=['Comment'])\n",
    "    \n",
    "def preprocess_comment(df):\n",
    "    df['Comment'] = df['Comment'].str.lower()\n",
    "    df = df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "    df['Comment'] = df['Comment'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "    df['Tokenized_Comment'] = df['Comment'].apply(nltk.tokenize.word_tokenize)\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    df['Tokenized_Comment'] = df['Tokenized_Comment'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    df['Tokenized_Comment'] = df['Tokenized_Comment'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    return df\n",
    "\n",
    "def score_comments(ID):\n",
    "    comments_df = get_comments(ID)\n",
    "    #comments_df = preprocess_comment(comments_df)\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    results = []\n",
    "    for index, row in comments_df.iterrows():\n",
    "        pol_score = sia.polarity_scores(row['Comment'])\n",
    "        comments_df.loc[index,'pos'] = pol_score['pos']\n",
    "        comments_df.loc[index,'neu'] = pol_score['neu']\n",
    "        comments_df.loc[index,'neg'] = pol_score['neg']\n",
    "        comments_df.loc[index,'compound'] = pol_score['compound']\n",
    "    out_values = []\n",
    "    out_values.append(comments_df['pos'].mean())\n",
    "    out_values.append(comments_df['neu'].mean())\n",
    "    out_values.append(comments_df['neg'].mean())\n",
    "    out_values.append(comments_df['compound'].mean())\n",
    "    out_values.append(len(comments_df.index))\n",
    "    return out_values\n",
    "\n",
    "for post in sub.hot(limit=num_posts):\n",
    "    if posts['PostID'].str.contains(post.id).any():\n",
    "        out_values = score_comments(post.id)\n",
    "        index = posts.index[posts['PostID'] == post.id].tolist()\n",
    "        posts.loc[index,'Avg_Pos'] = out_values[0]\n",
    "        posts.loc[index,'Avg_Neu'] = out_values[1]\n",
    "        posts.loc[index,'Avg_Neg'] = out_values[2]\n",
    "        posts.loc[index,'Avg_Compound'] = out_values[3]\n",
    "        posts.loc[index,'Num_Comments'] = out_values[4]\n",
    "posts = posts.sort_values(by=['Avg_Compound'], ascending=False)\n",
    "posts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f3f82003cc0ccb780dfb68465a2859b8753c3cce133cefccb0357f6f67295b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
